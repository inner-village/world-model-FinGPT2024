{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da97945-969a-4957-b1a3-2a1961731a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPSが利用可能です！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPSが利用可能です！\")\n",
    "else:\n",
    "    print(\"MPSは利用できません。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101e374d-789b-43d6-85a1-3bae50e09b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPSが利用可能です！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# MPSが利用可能か確認\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPSが利用可能です！\")\n",
    "else:\n",
    "    print(\"MPSは利用できません。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23476615-350a-4417-8317-73f647810241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imac27/FinGPT_2024/finGPT_env/lib/python3.10/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/imac27/FinGPT_2024/finGPT_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a40fc4d7f84dd880e4196e8d65d32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imac27/FinGPT_2024/finGPT_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory ./data/fingpt-forecaster-crypto-20230131-20231231-1-4-08/ not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# データのロード\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/fingpt-forecaster-crypto-20230131-20231231-1-4-08/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# テスト関数\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_demo\u001b[39m(model, tokenizer, prompt):\n",
      "File \u001b[0;32m~/FinGPT_2024/finGPT_env/lib/python3.10/site-packages/datasets/load.py:2210\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2208\u001b[0m fs, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m url_to_fs(dataset_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m   2209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m-> 2210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n\u001b[1;32m   2212\u001b[0m     posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   2213\u001b[0m ):\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory ./data/fingpt-forecaster-crypto-20230131-20231231-1-4-08/ not found"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk\n",
    "from peft import PeftModel\n",
    "from utils import *\n",
    "\n",
    "# デバイス設定\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# モデルのロード\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-7b-chat-hf',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float32,  # float16はサポートが限定的\n",
    ").to(device)\n",
    "\n",
    "# PeftModelの設定\n",
    "model = PeftModel.from_pretrained(base_model, 'FinGPT/fingpt-forecaster_dow30_llama2-7b_lora')\n",
    "model = model.eval().to(device)\n",
    "\n",
    "# トークナイザーの設定\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# データのロード\n",
    "test_dataset = load_from_disk('./data/fingpt-forecaster-crypto-20230131-20231231-1-4-08/')['test']\n",
    "\n",
    "# テスト関数\n",
    "def test_demo(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors='pt',\n",
    "        padding=False, max_length=4096\n",
    "    )\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # デバイスに移動\n",
    "\n",
    "    res = model.generate(\n",
    "        **inputs, max_length=4096, do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True\n",
    "    )\n",
    "    output = tokenizer.decode(res[0], skip_special_tokens=True)\n",
    "    return output\n",
    "\n",
    "answers, gts = [], []\n",
    "\n",
    "# 推論ループ\n",
    "for i in range(len(test_dataset)):\n",
    "    prompt = test_dataset[i]['prompt']\n",
    "    output = test_demo(model, tokenizer, prompt)\n",
    "    answer = re.sub(r'.*\\[/INST\\]\\s*', '', output, flags=re.DOTALL)\n",
    "    gt = test_dataset[i]['answer']\n",
    "    print('\\n------- Prompt ------\\n')\n",
    "    print(prompt)\n",
    "    print('\\n------- LLaMA2 Finetuned ------\\n')\n",
    "    print(answer)\n",
    "    print('\\n------- GPT4 Groundtruth ------\\n')\n",
    "    print(gt)\n",
    "    print('\\n===============\\n')\n",
    "    answers.append(answer)\n",
    "    gts.append(gt)\n",
    "\n",
    "calc_metrics(answers, gts)\n",
    "print(answers[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81bac8d-b27f-4b40-ac6d-9b3a44d1093d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02abc561afe1455780d112340d39902d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "32a2f19f34244485a8264dffa824b8d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_b8b0c3825d64456da4130ed29f800e4d",
       "max": 2,
       "style": "IPY_MODEL_8e7730aa1edc4e39b0fe0e0d3130fa99",
       "value": 2
      }
     },
     "3b5fdd8465bb493b809844088d9f744f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "491cd806ff204441ad6497f9b72dbe92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8e7730aa1edc4e39b0fe0e0d3130fa99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9b71c615a9f747c0975288901eb9de26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a3c24277659542cb957ead578264d7f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_491cd806ff204441ad6497f9b72dbe92",
       "style": "IPY_MODEL_9b71c615a9f747c0975288901eb9de26",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "b0b6be211eea44d9817abc596fcf0c28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_02abc561afe1455780d112340d39902d",
       "style": "IPY_MODEL_bb28bade7a19462d89050065ab7d6ef1",
       "value": " 2/2 [00:16&lt;00:00,  7.54s/it]"
      }
     },
     "b8b0c3825d64456da4130ed29f800e4d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bb28bade7a19462d89050065ab7d6ef1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c4a40fc4d7f84dd880e4196e8d65d32a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a3c24277659542cb957ead578264d7f7",
        "IPY_MODEL_32a2f19f34244485a8264dffa824b8d7",
        "IPY_MODEL_b0b6be211eea44d9817abc596fcf0c28"
       ],
       "layout": "IPY_MODEL_3b5fdd8465bb493b809844088d9f744f"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
